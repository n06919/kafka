## 개요

- **Kafka Streams란?**
    
    Apache Kafka에서 제공하는 **실시간 스트림 프로세싱을 위한 클라이언트 라이브러리**입니다. 
    
    Kafka 클러스터에서 데이터를 실시간으로 읽고 변환(Transformation), 집계(Aggregation), 필터링(Filtering)과 같은 처리를 통해 다른 토픽이나 데이터 저장소로 전달할 수 있습니다.
    
- **특징**
    - **실시간 스트림 처리**(Real-time processing)
        
        메시지가 들어오는 즉시 처리하여 결과를 바로 전달 가능
        
    - **상태 유지형 처리**(Stateful processing)
        
        상태 관리를 내장하고 있어 실시간 집계나 윈도우(Window) 연산이 용이함
        
    - **내결함성**(Fault tolerance)
        
        Kafka의 자체 기능을 통해 장애 발생 시 자동으로 복구 가능
        
    - **스케일 아웃**(Scalability)
        
        Kafka 클러스터의 파티션 수에 따라 자동으로 확장 가능
        
    - **배포 용이성**(Ease of deployment)
        
        별도의 스트림 처리 프레임워크나 추가 인프라 없이, 간단한 Java 애플리케이션으로 배포 가능
        

### Start Server

```bash
# Generate a Cluster UUID
$ KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
$ echo $KAFKA_CLUSTER_ID
# Format Log Directories
$ ./bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

# Start the Kafka Server
$ ./bin/kafka-server-start.sh config/kraft/server.properties

```

### Create Topic

```bash
cd ~/workspace/kafka_2.13-3.8.0

# create topic: streams-plaintext-input
$ ./bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-plaintext-input

# create topic: streams-wordcount-output
$ ./bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-wordcount-output \
    --config cleanup.policy=compact
    

# describe topic: streams-plaintext-input
$ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic streams-plaintext-input

# describe topic: streams-wordcount-output	
$ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic streams-wordcount-output
```

### WordCountDemo

### 개요:

`WordCountDemo.java`는 Kafka Streams DSL(도메인 특정 언어, Domain Specific Language)을 사용한 단어 카운팅 예제입니다. 

이 프로그램은 Kafka의 특정 토픽에서 데이터를 소비한 후, 각 단어의 발생 빈도를 계산한 뒤, 그 결과를 다시 Kafka 토픽에 출력합니다.

### 주요 특징:

- **Kafka Streams DSL**을 사용하여 간단하고 직관적으로 데이터 흐름을 정의.
- **KTable**과 같은 고수준 API를 사용하여 집계 작업을 수행.
- 코드는 복잡하지 않으며, 데이터 스트림 처리의 기본적인 작업을 보여줍니다.

### 흐름:

- 스트림으로부터 데이터 읽기
- 각 문장을 단어로 분리
- 각 단어에 대한 카운트를 계산
- 결과를 출력 토픽에 쓰기

### 사용 시나리오:

- Kafka Streams API를 이용한 단순한 단어 카운팅.

```bash
cd ~/workspace

git clone https://github.com/n06919/kafka-stream-exam.git
export USER_DIR=/config/workspace
export CLASSPATH=$USER_DIR/kafka-stream-exam/build/classes/java/main:${CLASSPATH#:}

cd /config/workspace/kafka-stream-exam
./gradlew build

cd $USER_DIR/kafka-stream-exam/build/classes/java/main/com/skcc/college/streams
$USER_DIR/kafka_2.13-3.8.0/bin/kafka-run-class.sh com.skcc.college.streams.WordCountDemo

# 새창
# kafka home
cd /config/workspace/kafka_2.13-3.8.0
**./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic streams-wordcount-output \
    --from-beginning \
    --property print.key=true \
    --property print.value=true \
    --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
    --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer**

# 새창
# kafka home
cd /config/workspace/kafka_2.13-3.8.0
**./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic streams-plaintext-input
>all streams lead to kafka

# consumer
all	    1
streams	1
lead	  1
to	    1
kafka 	1

# producer
> hello kafka streams

# consumer
all	    1
streams	1
lead	  1
to	    1
kafka	  1
hello	  1
kafka	  2
streams	2**
```

### WordCountProcessorDemo

### 개요:

`WordCountProcessorDemo.java`는 Kafka Streams의 Processor API를 사용하여 직접 단어 카운팅 처리를 구현한 예제입니다. 

Processor API는 DSL보다 더 낮은 수준의 API로, 데이터 처리 흐름을 좀 더 세밀하게 제어할 수 있습니다.

### 주요 특징:

- **Processor API**를 사용하여 데이터 처리를 사용자 정의할 수 있음.
- 각 단어 카운팅 로직이 사용자 정의 Processor 안에 직접 구현.
- **Topology**를 수동으로 구성하여 스트림의 처리 경로를 명시적으로 설정.

### 흐름:

- 스트림에서 데이터를 읽고 Processor를 통해 각 문장을 처리
- 사용자 정의 Processor에서 문장을 단어로 분리하고 카운팅
- 카운트 결과를 저장하거나 출력

### 사용 시나리오:

- 데이터 처리의 세부 제어가 필요한 상황에서 유용.
- DSL이 제공하는 추상화된 구조보다 더 유연한 처리를 하고 싶은 경우 적합.

https://chatgpt.com/share/67e23799-50c8-800b-9cbd-ae67ced53db3

```bash
export USER_DIR=/config/workspace
export CLASSPATH=$USER_DIR/kafka-stream-exam/build/classes/java/main:${CLASSPATH#:}
cd $USER_DIR/kafka-stream-exam/build/classes/java/main/com/skcc/college/streams
$USER_DIR/kafka_2.13-3.8.0/bin/kafka-run-class.sh com.skcc.college.streams.WordCountProcessorDemo

----------- 1726823099750 ----------- 
[, 1]
[all, 1]
[hello, 4]
[kafka, 5]
[lead, 1]
[streams, 5]
[to, 1]

----------- 1726823220622 ----------- 
[, 1]
[all, 2]
[hello, 5]
[kafka, 7]
[lead, 2]
[streams, 7]
[to, 2]
```

### WordCountTransformerDemo

### 개요:

`WordCountTransformerDemo.java`는 Kafka Streams에서 **Transformer API**를 사용하여 단어 카운팅을 구현한 예제입니다. 

Transformer API는 Processor API와 유사하지만, 조금 더 간결한 형태로 상태를 유지하면서 스트림을 처리할 수 있습니다.

### 주요 특징:

- **Transformer API**를 사용하여 스트림의 각 레코드에 대해 변환 작업을 수행.
- 상태 저장소(State Store)를 직접 사용하여 처리 중에 상태를 관리.
- `transform()` 메서드를 사용해 데이터를 변환하며, 이 과정에서 상태를 유지할 수 있음.

### 흐름:

- 스트림에서 데이터를 읽고 Transformer로 변환 작업을 수행
- 각 문장을 단어로 나누고 카운트
- 상태 저장소에 카운트를 저장하며 처리

### 사용 시나리오:

- 상태 저장이 필요한 복잡한 데이터 처리 작업에서 유용.
- Processor API와 유사하지만, 상태 저장소와의 통합이 좀 더 간편하게 이루어짐.

```bash
export USER_DIR=/config/workspace
export CLASSPATH=$USER_DIR/kafka-stream-exam/build/classes/java/main:${CLASSPATH#:}
cd $USER_DIR/kafka-stream-exam/build/classes/java/main/com/skcc/college/streams
$USER_DIR/kafka_2.13-3.8.0/bin/kafka-run-class.sh com.skcc.college.streams.WordCountTransformerDemo
----------- 1726823220622 ----------- 
[, 1]
[all, 2]
[hello, 5]
[kafka, 7]
[lead, 2]
[streams, 7]
[to, 2]
```


Kafka Streams의 세 가지 구현 방식 장단점 비교
| 구현 방식 | 장점 | 단점 |
|-----------|------|------|
| WordCountDemo<br>(고수준 DSL API) | • 간결한 코드 (4줄로 워드 카운트 구현)<br>• 높은 가독성<br>• 쉬운 유지보수<br>• 복잡한 로직 추상화<br>• 다중 파티션 토픽 지원<br>• 낮은 학습 곡선<br>• 빠른 개발 속도 | • 유연성 제한<br>• 내부 동작 방식 가시성 부족<br>• 세부적인 제어 어려움<br>• 복잡한 사용자 정의 로직 구현의 한계 |
| WordCountProcessorDemo<br>(저수준 Processor API) | • 세부적인 제어 가능<br>• 스케줄링 기능 활용<br>• 상태 저장소 직접 접근/관리<br>• 토폴로지 직접 설계 가능<br>• 맞춤형 처리 로직 구현<br>• 시간 기반 연산 처리 용이 | • 코드 복잡성 증가<br>• 가독성 저하<br>• 유지보수 어려움<br>• 단일 파티션 토픽만 지원<br>• 높은 학습 곡선<br>• 오류 발생 가능성 증가<br>• 개발 시간 증가 |
| WordCountTransformerDemo<br>(하이브리드 방식) | • DSL의 편의성과 Processor API의 유연성 결합<br>• StreamsBuilder 활용 가능<br>• 세부 처리 로직 커스터마이징<br>• 코드 캡슐화 개선<br>• 상태 저장소 직접 관리 가능<br>• DSL과 Processor API 간 유연한 전환 | • DSL보다 복잡한 구현<br>• 단일 파티션 토픽만 지원<br>• 중간 수준의 학습 곡선<br>• 두 API 스타일 혼합으로 인한 일관성 부족<br>• 두 API에 대한 이해 필요<br>• 유지보수 복잡성 |


사용 권장 상황
| 구현 방식 | 권장 사용 상황 |
|-----------|---------------|
| WordCountDemo<br>(고수준 DSL API) | • 간단한 스트림 처리 작업<br>• 빠른 개발이 필요한 경우<br>• 표준 변환 및 집계 작업<br>• 다중 파티션 처리가 필요한 경우<br>• 팀의 카프카 스트림 경험이 제한적인 경우 |
| WordCountProcessorDemo<br>(저수준 Processor API) | • 복잡한 상태 관리가 필요한 경우<br>• 세부적인 제어가 중요한 경우<br>• 시간 기반 처리 로직 구현<br>• 고도로 사용자 정의된 처리 로직<br>• 내부 작동 방식에 대한 완전한 제어가 필요한 경우 |
| WordCountTransformerDemo<br>(하이브리드 방식) | • 대부분은 DSL로 구현하되 특정 부분만 저수준 API가 필요한 경우<br>• 고수준 API와 저수준 API의 장점을 모두 활용해야 하는 경우<br>• 기존 DSL 파이프라인에 사용자 정의 로직 통합이 필요한 경우 |

이 표는 세 가지 Kafka Streams 구현 방식의 주요 장단점과 적합한 사용 상황을 요약하여 보여줍니다. 실제 프로젝트에서는 요구사항, 성능 목표, 팀의 기술 수준을 고려하여 적절한 방식을 선택하는 것이 중요합니다.